---
title: "Assignment 2 - Methods 4"
author: "Laurits Lyngbaek"
date: "2025-03-18"
output: html_document
---
# Second assignment
The second assignment uses chapter 3, 5 and 6. The focus of the assignment is getting an understanding of causality.

##  Chapter 3: Causal Confussion
**Reminder: We are tying to estimate the probability of giving birth to a boy**
I have pasted a working solution to questions 6.1-6.3 so you can continue from here:)

**3H3**
Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). 

```{r}
# 3H1
# Find the posterior probability of giving birth to a boy:
pacman::p_load(rethinking)
data(homeworkch3)
set.seed(1)
W <- sum(birth1) + sum(birth2)
N <- length(birth1) + length(birth2)
p_grid <-seq(from =0, to = 1, len =1000)
prob_p <- rep(1,1000)
prob_data <- dbinom(W,N,prob=p_grid)
posterior <-prob_data * prob_p
posterior <- posterior / sum(posterior)

# 3H2
# Sample probabilities from posterior distribution:
samples <- sample (p_grid, prob = posterior, size =1e4, replace =TRUE)

# 3H3
# Simulate births using sampled probabilities as simulation input, and check if they allign with real value.
simulated_births <- rbinom(n = 1e4, size = N, prob = samples)
rethinking::dens(simulated_births,show.PI = 0.95)
abline(v=W, col="red")
title("Simulated amount of boys in 200 births - red line is real value")

```

**3H4.**
Now compare 10,000 counts of boys from 100 simulated firstborns only to the number of boys in the first births, birth1. How does the model look in this light?
```{r}
simulated_first_births <- rbinom(n = 1e4, size = length(birth1), prob = samples)
W1 <- sum(birth1)

rethinking::dens(simulated_first_births,show.PI = 0.95)
abline(v=W1, col="red")

```
The model seems to overestimate the number of firstborn boys.

**3H5.** 
The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to cound the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?
```{r}
n_first_born_girls <- -sum(birth1-1)

simulated_birth_after_girl <- rbinom(1e4, size = n_first_born_girls, prob = samples) # Simulate boys born after a girl
W2 <- sum(birth2[birth1 == 0]) # Get number of boys born after a girl

rethinking::dens(simulated_birth_after_girl,show.PI = 0.95)
abline(v=W2, col="red")

```
We know that there was 49 firstborn girls. We then simulate n=49 births 10,000 times, i.e. we simulate the 49 second-births, this simulation is based on all the data and does not care whether the birth was a first or a second birth. By comparing to the known second-births we can see the model is underestimating the number of boys born after a firstborn girl.

## Chapter 5: Spurrious Correlations
Start of by checking out all the spurious correlations that exists in the world.
Some of these can be seen on this wonderful website: https://www.tylervigen.com/spurious/random
All the medium questions are only asking you to explain a solution with words, but feel free to simulate the data and prove the concepts.


**5M1**.
Invent your own example of a spurious correlation. An outcome variable should be correlated
with both predictor variables. But when both predictors are entered in the same model, the correlation
between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).

```{r}
# Number of students
n <- 1000

# Simulate students
lecture_attendance <- rnorm(n)
happiness <- rnorm(n, lecture_attendance)
grade <- rnorm(n, lecture_attendance)

# Put the data in a data frame
d <- data.frame(lecture_attendance, happiness, grade)

# Model 1: grade ~ happiness
m5.1.1 <- quap(
  alist(
    grade ~ dnorm(mu, sigma),
    mu <- a + b_h * happiness,
    a ~ dnorm(0, 0.5),
    b_h ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Model 2: grade ~ lecture_attendance
m5.1.2 <- quap(
  alist(
    grade ~ dnorm(mu, sigma),
    mu <- a + b_la * lecture_attendance,
    a ~ dnorm(0, 0.5),
    b_la ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Model 3: grade ~ happiness + lecture_attendance
m5.1.3 <- quap(
  alist(
    grade ~ dnorm(mu, sigma),
    mu <- a + b_h * happiness + b_la * lecture_attendance,
    a ~ dnorm(0, 0.5),
    b_h ~ dnorm(0, 0.5),
    b_la ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Look at the parameter estimates
precis(m5.1.1)
precis(m5.1.2)
precis(m5.1.3)
```
In this simulation lecture attendance is the common cause of both happiness and grade.
When you analyze grade ~ happiness, you'll see a positive correlation
When you analyze grade ~ lecture_attendance, you'll see a positive correlation
When you analyze grade ~ happiness + lecture_attendance, the effect of happiness disappears (except for a small amount of noise)

**5M2**.
Invent your own example of a masked relationship. An outcome variable should be correlated
with both predictor variables, but in opposite directions. And the two predictor variables should be
correlated with one another.

```{r}
n <- 1000

time_at_party <- rnorm(n) # 
beers_drunk <- rnorm(n, time_at_party)  
people_talked_to <- rnorm(n, time_at_party)  # Negative correlation with study_hours

chance_of_getting_friends <- rnorm(n, (people_talked_to - beers_drunk) / 2)

d <- data.frame(chance_of_getting_friends, beers_drunk, people_talked_to)

# Predict chance_of_getting_friends ~ beers_drunk
m5.2.1 <- quap(
  alist(
    chance_of_getting_friends ~ dnorm(mu, sigma),
    mu <- a + b_b * beers_drunk,
    a ~ dnorm(0, 0.5),
    b_b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Predict chance_of_getting_friends ~ people_talked_to
m5.2.2 <- quap(
  alist(
    chance_of_getting_friends ~ dnorm(mu, sigma),
    mu <- a + b_p * people_talked_to,
    a ~ dnorm(0, 0.5),
    b_p ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Predict chance_of_getting_friends ~ beers_drunk + people_talked_to
m5.2.3 <- quap(
  alist(
    chance_of_getting_friends ~ dnorm(mu, sigma),
    mu <- a + b_b * beers_drunk + b_p * people_talked_to,
    a ~ dnorm(0, 0.5),
    b_b ~ dnorm(0, 0.5),
    b_p ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

precis(m5.2.1)
precis(m5.2.2)
precis(m5.2.3)
```
In this simulation a person spends some amount of time at a party, during that time they both drink beers and talk to people, the more people they talk to the more likely they find new friends, however, when they become more drunk they also decrease the likelihood of making new friends.
When you analyze chance_of_getting_friends ~ beers_drunk, you'll see a negative correlation
When you analyze chance_of_getting_friends ~ people_talked_to, you'll see a positive correlation
When you analyze chance_of_getting_friends ~ beers_drunk + people_talked_to, their individual effects are exaggerated (i.e. overestimated).

**5M3**.
It is sometimes observed that the best predictor of fire risk is the presence of firefighters—
States and localities with many firefighters also have more fires. Presumably firefighters do not cause
fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the
same reversal of causal inference in the context of the divorce and marriage data. How might a high
divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using
multiple regression

People getting divorced would increase the number of single people. With more single people the likelihood of two people finding each other and getting married would increase. The implication is that people get married more than once, thus it could be evaluated by tracking the amount of people getting married for the second time. If that parameter is added the coefficient for divorce should be closer to 0, than when the regression isn't:
`marriage ~ a*divorce` -> a is higher
`marriage ~ a*divorce + b*rate_of_remarriage*` -> a is closer to 0


**5M5**.
One way to reason through multiple causation hypotheses is to imagine detailed mechanisms
through which predictor variables may influence outcomes. For example, it is sometimes argued that
the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome
variable). However, there are at least two important mechanisms by which the price of gas could
reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to
less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals.
Can you outline one or more multiple regressions that address these two mechanisms? Assume you
can have any predictor data you need.

Variables:
- G = gas price
- O = obesity rate
- D = driving
- E = exercise
- R = eating at restaurants

Mutual relationships:
As G increases D decreases

Model 1 (exercise):
As D decreases E increases
As E increases O decreases

Model 2 (restaurants):
As D decreases R decreases
As R decreases O decreases

This could be modeled as a set of regressions with varying predictors and outcomes i.e. the effect by mediation through each of the routes. The models would be as follows:
- D ~ G
- E ~ D
- R ~ D
- O ~ E
- O ~ R
- O ~ G

## Chapter 5: Foxes and Pack Sizes  
All five exercises below use the same data, data(foxes) (part of rethinking).84 The urban fox (Vulpes vulpes) is a successful exploiter of human habitat. Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included. The data frame has five columns:
(1) group: Number of the social group the individual fox belongs to
(2) avgfood: The average amount of food available in the territory
(3) groupsize: The number of foxes in the social group
(4) area: Size of the territory
(5) weight: Body weight of the individual fox

**5H1.** 
Fit two bivariate Gaussian regressions, using quap: (1) body weight as a linear function of territory size (area), and (2) body weight as a linear function of groupsize. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?
```{r}
data(foxes)

# Model 1: weight ~ area
m5H1.1 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_a * area,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_a ~ dnorm(3, 3), # No idea about area but the data shows it is somewhere between 1 and 5
    sigma ~ dexp(1)
  ), data = foxes
)

m5H1.2 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_g * groupsize,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_g ~ dnorm(6, 3), # No idea about area but the data shows it is somewhere between 2-8
    sigma ~ dexp(1)
  ), data = foxes
)
```


```{r}
# Plot the results
weight.seq <- seq(from = min(foxes$weight), to = max(foxes$weight), length.out = 100)
mu <- link(m5H1.1, data = list(area = weight.seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.95)

plot(foxes$weight, foxes$area, col = col.alpha(rangi2, 0.5), xlab = "Weight", ylab = "Area")
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)

```
```{r}
mu2 <- link(m5H1.2, data = list(groupsize = weight.seq))
mu2.mean <- apply(mu2, 2, mean)
mu2.PI <- apply(mu2, 2, PI, prob = 0.95)

plot(foxes$weight, foxes$groupsize, col = col.alpha(rangi2, 0.5), xlab = "Weight", ylab = "Groupsize")
lines(weight.seq, mu2.mean)
shade(mu2.PI, weight.seq)

```
Both regression lines are near horizontal, indicating that neither area nor groupsize are important for predicting fox body weight.

**5H2.**
Now fit a multiple linear regression with weight as the outcome and both area and groupsize as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?
```{r}
m5H2 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_a * area + b_g * groupsize,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_a ~ dnorm(3, 2), # No idea about area but the data shows it is somewhere between 1-5
    b_g ~ dnorm(6, 2), # No idea about area but the data shows it is somewhere between 2-8
    sigma ~ dexp(1)
  ), data = foxes
)

precis(m5H2)
```

```{r}
weight.seq <- seq(from = min(foxes$weight), to = max(foxes$weight), length.out = 100)

area_seq <- seq(from = min(foxes$area), to = max(foxes$area), length.out = 100)
groupsize_seq <- seq(from = min(foxes$groupsize), to = max(foxes$groupsize), length.out = 100)

pred_area_data <- data.frame(area = area_seq, groupsize = mean(foxes$groupsize))
pred_groupsize_data <- data.frame(area = mean(foxes$area), groupsize = groupsize_seq)

pred_area <- link(m5H2, data = pred_area_data)
pred_groupsize <- link(m5H2, data = pred_groupsize_data)

# Compute mean and intervals from the matrices
pred_area_mu <- apply(pred_area, 2, mean)
pred_area_PI <- apply(pred_area, 2, PI)

pred_groupsize_mu <- apply(pred_groupsize, 2, mean)
pred_groupsize_PI <- apply(pred_groupsize, 2, PI)

plot(NULL, xlim = range(area_seq), ylim = range(pred_area_PI),
     xlab = "Territory area", ylab = "Predicted weight",
     main = "Effect of Area (holding groupsize constant)")
lines(area_seq, pred_area_mu, lwd = 2)
shade(pred_area_PI, area_seq)

plot(NULL, xlim = range(groupsize_seq), ylim = range(pred_groupsize_PI),
     xlab = "Groupsize", ylab = "Predicted weight",
     main = "Effect of Groupsize (holding area constant)")
lines(groupsize_seq, pred_groupsize_mu, lwd = 2)
shade(pred_groupsize_PI, groupsize_seq)
```
The model shows that the two variables are nearly equally important for predicting fox body weight, however they have inverse effects. The two variables are correlated, so when we counter-factually hold one variable constant (i.e. by stratifying by it), then the other variables influence is not masked by the other variable.


**5H3.**
Finally, consider the avgfood variable. Fit two more multiple regressions: (1) body weight as an additive function of avgfood and groupsize, and (2) body weight as an additive function of all three variables, avgfood and groupsize and area. Compare the results of these models to the previous models you’ve fit, in the first two exercises. (a) Is avgfood or area a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose. (b) When both avgfood or area are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?
```{r}
m5H3.1 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_a * area + b_f * avgfood,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_a ~ dnorm(3, 2), # No idea about area but the data shows it is somewhere between 1-5
    b_f ~ dnorm(1, 1), # No idea about area but the data shows it is somewhere between 0-2
    sigma ~ dexp(1)
  ), data = foxes
)

m5H3.2 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_a * area + b_g * groupsize + b_f * avgfood,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_a ~ dnorm(3, 2), # No idea about area but the data shows it is somewhere between 1-5
    b_g ~ dnorm(6, 2), # No idea about area but the data shows it is somewhere between 2-8
    b_f ~ dnorm(1, 1), # No idea about area but the data shows it is somewhere between 0-2
    sigma ~ dexp(1)
  ), data = foxes
)

precis(m5H3.1)
precis(m5H3.2)
```

a) We see that in the model weight ~ area + avgfood, both coefficients are roughly 0. This suggests that there is a high correlation between area and avgfood that is causing a masked relationship. The other model weight ~ area + groupsize + avgfood shows shows area and groupsize have coefficient sizes of roughly 0.5 and -0.5 respectively, while avgfood has a coefficient closer to 1.3. The larger magnitude suggest that avgfood is a better predictor of body weight than area. This also assumes that they are on similar scales, as they are not standardized. However an argument could be made that area is a more certain predictor of body weight as its range of estimates are more narrow, so it depends on the interpretation of "better" (i.e. does better mean more certain or larger effect). 

b) The standard errors increase because they are correlated and thus have shared variance. When we add a variable that is correlated with another variable, the model has to estimate the effect of both variables at the same time. This leads to larger standard errors because the model is less certain about the individual effects of each variable.


**Defining our theory with explicit DAGs**
Assume this DAG as an causal explanation of fox weight:
```{r}
pacman::p_load(dagitty,
               ggdag)
dag <- dagitty('dag {
A[pos="1.000,0.500"]
F[pos="0.000,0.000"]
G[pos="2.000,0.000"]
W[pos="1.000,-0.500"]
A -> F
F -> G
F -> W
G -> W
}')

# Plot the DAG
ggdag(dag, layout = "circle")+
  theme_dag()
```
where A is area, F is avgfood, G is groupsize, and W is weight. 

**Using what you know about DAGs from chapter 5 and 6, solve the following three questions:**

1) Estimate the total causal influence of A on F. What effect would increasing the area of a territory have on the amount of food inside of it?
```{r}
# Fit a model to estimate the effect of area on avgfood
m5H4.1 <- quap(
  alist(
    avgfood ~ dnorm(mu, sigma),
    mu <- a + b_a * area,
    a ~ dnorm(1, 1), # We know there is likely food where foxes live, so let's bias towards positive.
    b_a ~ dnorm(0, 1),  # We don't really know the effect of area on food.
    sigma ~ dexp(1)
  ), data = foxes
)
precis(m5H4.1)
```

Increasing the area of a territory by 1 unit would increase the amount of food by 0.19 units on average (there is almost no uncertainty for this parameter).


2) Infer the **total** causal effect of adding food F to a territory on the weight W of foxes. Can you calculate the causal effect by simulating an intervention on food?
```{r}
# Fit a model to estimate the effect of avgfood on weight
m5H4.2 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_f * avgfood,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_f ~ dnorm(1, 1), # No idea about area but the data shows it is somewhere between 0-2
    sigma ~ dexp(1)
  ), data = foxes
)

precis(m5H4.2)
```

The total effect is uncertain as the MAP is close to zero (0.07), but the estimates vary wildly between -0.7 and 0.84 (at 5.5% and 94% quantiles of estimates). This suggests that the total effect of food on weight is not very strong.

3) Infer the **direct** causal effect of adding food F to a territory on the weight W of foxes. In light of your estimates from this problem and the previous one, what do you think is going on with these foxes? 

```{r}
# Fit a model to estimate the direct effect of avgfood on weight
# We stratify by gropusize.

m5H4.3 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + b_f * avgfood + b_g * groupsize,
    a ~ dnorm(7, 3), # Google says foxes weigh 2-14 kg
    b_f ~ dnorm(1, 1), # No idea about area but the data shows it is somewhere between 0-2
    b_g ~ dnorm(6, 2), # No idea about area but the data shows it is somewhere between 2-8
    sigma ~ dexp(1)
  ), data = foxes
)
precis(m5H4.3)
```
The direct effect of food on weight (i.e. stratified by groupsize) is MAP 1.99 (5.5% quantile 0.75, 94% quantile 3.23). This suggests that the direct effect of food on weight is positive. 

What is likely happening:
- More food in a territory directly increases the weight of foxes.
- More food also attracts more foxes to the territory (increases groupsize)
- Larger groupsize increases competition for food, which decreases the individual weight of foxes.
- This nets a near-zero total effect of food on weight.

This relationship is hidden by post-treatment bias, where the groupsize can be considered a post-treatment variable.

## Chapter 6: Investigating the Waffles and Divorces
**6H1**. 
Use the Waffle House data, data(WaffleDivorce), to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph.

```{r}
# Load data and standardize
data(WaffleDivorce)

d <- WaffleDivorce

# Standardize the data
d$D <- standardize(d$Divorce)
d$W <- standardize(d$WaffleHouses)
d$S <- standardize(d$South)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)

# The chapter provides a DAG for this data:
waffle_dag <- dagitty("dag {
  A -> D
  A -> M -> D
  A <- S -> M
  S -> W -> D
}")

# Fix coordinates
coordinates(waffle_dag) <- list(x = c(A = 1, S = 1, M = 2, W = 3, D = 3),
                                y = c(A = 1, S = 3, M = 2, W = 3, D = 1))
# Plot the DAG
ggdag(waffle_dag, layout = "circle") +
  theme_dag()
```
```{r}
# Find the adjustment sets
adjustmentSets(waffle_dag, exposure = "W", outcome = "D")
```
```{r}
# Model D ~ W + S

m6H1 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + b_w * W + b_s * S,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_w ~ dnorm(0, 0.5), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

precis(m6H1)
```
WaffleHouses has MAP 0.05 (-0.21 to 0.32 at 5.5% and 94.5% quantiles). This suggests that the total effect of WaffleHouses on Divorce is roughly zero, however with uncertainty to both positive and negative sides. Most of the variance is taken up by being in the south (MAP 0.29, 0.02 to 0.55 at 5.5% and 94.5% quantiles).

**6H2**. 
Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren't in the data.
```{r}
# Get the implied conditional independencies
impliedConditionalIndependencies(waffle_dag)
```
```{r}
# Test A _||_ W | S
# A ~ S
m6H2.1 <- quap(
  alist(
    A ~ dnorm(mu, sigma),
    mu <- a + b_s * S,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

# A ~ S + W
m6H2.2 <- quap(
  alist(
    A ~ dnorm(mu, sigma),
    mu <- a + b_s * S + b_w * W,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    b_w ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

precis(m6H2.1)
precis(m6H2.2)
```
We see that including W in the model slightly lowers the MAP of S from -0.23 to -0.27 while the MAP of W is 0.06 but with a large uncertainty (-0.21 to 0.34 at 5.5% and 94.5% quantiles). This suggests that the model is not very sensitive to the inclusion of W, and thus the independence assumption mostly holds.

```{r}
# Test D _||_ S | A, M, W

# D ~ A + M + W
m6H2.3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + b_a * A + b_m * M + b_w * W,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_a ~ dnorm(0, 0.5), # Everything is standardized
    b_m ~ dnorm(0, 0.5), # Everything is standardized
    b_w ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

# D ~ A + M + W + S
m6H2.4 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + b_a * A + b_m * M + b_w * W + b_s * S,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_a ~ dnorm(0, 0.5), # Everything is standardized
    b_m ~ dnorm(0, 0.5), # Everything is standardized
    b_w ~ dnorm(0, 0.5), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)


precis(m6H2.3)
precis(m6H2.4)
```

We see that including S in the model lowers the MAP of W from 0.18 to 0.09. The rest other estimates hardly change. The MAP of S is 0.15 but with a large uncertainty (-0.09 to 0.38 at 5.5% and 94.5% quantiles). This suggests that the model is somewhat sensitive to the inclusion of S, and thus the independence assumption does not hold.


```{r}
# Test M _||_ W | S

# M ~ S
m6H2.5 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + b_s * S,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

# M ~ S + W
m6H2.6 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + b_s * S + b_w * W,
    a ~ dnorm(0, 0.2), # Everything is standardized
    b_s ~ dnorm(0, 0.5), # Everything is standardized
    b_w ~ dnorm(0, 0.5), # Everything is standardized
    sigma ~ dexp(1) # sd must be positive
  ), data = d
)

precis(m6H2.5)
precis(m6H2.6)
```
We see that including W in the model leads to a small increase on the MAP of S from 0.08 to 0.10. The MAP for W is -0.04, however for both S and W the uncertainty is quite large (-18 to 0.38 and -0.32 to 0.24 respectively at 5.5% and 94.5% quantiles). This suggests that the model is not very sensitive to the inclusion of W, and thus the independence assumption mostly holds.

These three tests of conditional independencies suggest that we should reconsider the DAG. Especially the independence assumption of D _||_ S | A, M, W. This suggests there are other confounding variables between divorce and being in the Southern USA. One major unobserved confounder could be religion, which is prevalent in that region of the USA, while also having specific values regarding marriage/divorce tied in. Generally adding nodes and thus more arrows to a DAG is likely to make it more realistic with regard to the causal relationships (if done with some insight, not blindly/naively). This risk sacrificing practicality and explainability to gain realism.


